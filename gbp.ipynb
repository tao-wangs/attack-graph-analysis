{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as n\n",
    "import torch\n",
    "import random\n",
    "import os \n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Callable, Optional, Union\n",
    "\n",
    "class Gaussian:\n",
    "    def __init__(self, dim, eta, lam, type):\n",
    "        self.dim = dim \n",
    "        self.eta = eta \n",
    "        self.lam = lam \n",
    "\n",
    "    def mean(self):\n",
    "        return torch.matmul(torch.inverse(self.lam), self.eta)\n",
    "    \n",
    "    def cov(self):\n",
    "        return torch.inverse(self.lam)\n",
    "    \n",
    "    def mean_and_cov(self):\n",
    "        cov = self.cov()\n",
    "        mean = torch.matmul(cov, self.eta)\n",
    "        return [mean, cov]\n",
    "\n",
    "    def set_with_cov_form(self, mean, cov):\n",
    "        self.lam = torch.inverse(cov)\n",
    "        self.eta = self.lam @ mean "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GBPSettings:\n",
    "    def __init__(self, \n",
    "                 damping, \n",
    "                 beta, \n",
    "                 num_undamped_iters,\n",
    "                 min_linear_iters, \n",
    "                 dropout,\n",
    "                 reset_iters_since_relin,\n",
    "                 type):\n",
    "        \n",
    "        self.damping = damping\n",
    "        self.num_undamped_iters = num_undamped_iters\n",
    "        self.dropout = dropout \n",
    "        self.beta = beta\n",
    "        self.min_linear_iters = min_linear_iters\n",
    "        self.reset_iters_since_relin = reset_iters_since_relin\n",
    "\n",
    "    def get_damping(self, iters_since_relin):\n",
    "        if iters_since_relin > self.num_undamped_iters:\n",
    "            return self.damping\n",
    "        else:\n",
    "            return 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "GBPSettings.__init__() missing 7 required positional arguments: 'damping', 'beta', 'num_undamped_iters', 'min_linear_iters', 'dropout', 'reset_iters_since_relin', and 'type'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mFactorGraph\u001b[39;00m:\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, gbp_settings \u001b[38;5;241m=\u001b[39m GBPSettings()):\n\u001b[0;32m      3\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvar_nodes \u001b[38;5;241m=\u001b[39m []\n",
      "Cell \u001b[1;32mIn[9], line 2\u001b[0m, in \u001b[0;36mFactorGraph\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mFactorGraph\u001b[39;00m:\n\u001b[1;32m----> 2\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, gbp_settings \u001b[38;5;241m=\u001b[39m \u001b[43mGBPSettings\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m):\n\u001b[0;32m      3\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvar_nodes \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      4\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfactors \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[1;31mTypeError\u001b[0m: GBPSettings.__init__() missing 7 required positional arguments: 'damping', 'beta', 'num_undamped_iters', 'min_linear_iters', 'dropout', 'reset_iters_since_relin', and 'type'"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "class FactorGraph:\n",
    "    def __init__(self, gbp_settings = GBPSettings()):\n",
    "        self.var_nodes = []\n",
    "        self.factors = []\n",
    "        self.gbp_settings = gbp_settings\n",
    "\n",
    "    def add_var_node(self, dofs, prior_mean, prior_diag_cov, properties):\n",
    "        variableID = len(self.var_nodes)\n",
    "        self.var_nodes.append(VariableNode(variableID, dofs, properties))\n",
    "        prior_cov = torch.zeros(dofs, dofs, dtype=prior_diag_cov.dtype)\n",
    "        prior_cov[range(dofs), range(dofs)] = prior_diag_cov\n",
    "        self.var_nodes[-1].prior.set_with_cov_form(prior_mean, prior_cov)\n",
    "        self.var_nodes[-1].update_belief()\n",
    "\n",
    "    def add_factor(self, adj_var_ids, measurement, meas_model, properties):\n",
    "        factorID = len(self.factors)\n",
    "        adj_var_nodes = [self.var_nodes[i] for i in adj_var_ids]\n",
    "        self.factors.append(Factor(factorID, adj_var_ids, measurement, meas_model, properties))\n",
    "        for var in adj_var_nodes:\n",
    "            var.adj_factors.append(self.factors[-1])\n",
    "\n",
    "    def update_all_beliefs(self):\n",
    "        for var_node in self.var_nodes:\n",
    "            var_node.update_belief()\n",
    "    \n",
    "    def compute_all_messages(self, apply_dropout=True):\n",
    "        for factor in self.factors:\n",
    "            if apply_dropout and random.random() > self.gbp_settings.dropout or not apply_dropout:\n",
    "                damping = self.gbp_settings.get_damping(factor.iters_since_relin)\n",
    "                factor.compute_messages(damping)\n",
    "    \n",
    "    def linearise_all_factors(self) -> None:\n",
    "        for factor in self.factors:\n",
    "            factor.compute_factor()\n",
    "\n",
    "    def jit_linearisation(self) -> None:\n",
    "        \"\"\"\n",
    "            Check for all factors that the current estimate is close to the linearisation point.\n",
    "            If not, relinearise the factor distribution.\n",
    "            Relinearisation is only allowed at a maximum frequency of once every min_linear_iters iterations.\n",
    "        \"\"\"\n",
    "        for factor in self.factors:\n",
    "            if not factor.meas_model.linear:\n",
    "                adj_belief_means = factor.get_adj_means()\n",
    "                factor.iters_since_relin += 1\n",
    "                if torch.norm(factor.linpoint - adj_belief_means) > self.gbp_settings.beta and factor.iters_since_relin >= self.gbp_settings.min_linear_iters:\n",
    "                    factor.compute_factor()\n",
    "    \n",
    "    def synchronous_iteration(self) -> None:\n",
    "        self.robustify_all_factors()\n",
    "        self.jit_linearisation()  # For linear factors, no compute is done\n",
    "        self.compute_all_messages()\n",
    "        self.update_all_beliefs()\n",
    "\n",
    "    \n",
    "    # THIS IS WHERE THE MAGIC HAPPENS\n",
    "    def gbp_solve(self, n_iters: Optional[int] = 20, converged_threshold: Optional[float] = 1e-6, include_priors: bool = True) -> None:\n",
    "        energy_log = [self.energy()]\n",
    "        print(f\"\\nInitial Energy {energy_log[0]:.5f}\")\n",
    "        i = 0\n",
    "        count = 0\n",
    "        not_converged = True\n",
    "        while not_converged and i < n_iters:\n",
    "            self.synchronous_iteration()\n",
    "            if i in self.gbp_settings.reset_iters_since_relin:\n",
    "                for f in self.factors:\n",
    "                    f.iters_since_relin = 1\n",
    "\n",
    "            energy_log.append(self.energy(include_priors=include_priors))\n",
    "            print(\n",
    "                f\"Iter {i+1}  --- \"\n",
    "                f\"Energy {energy_log[-1]:.5f} --- \"\n",
    "                # f\"Belief means: {self.belief_means().numpy()} --- \"\n",
    "                # f\"Robust factors: {[factor.meas_model.loss.robust() for factor in self.factors]}\"\n",
    "                # f\"Relins: {sum([(factor.iters_since_relin==0 and not factor.meas_model.linear) for factor in self.factors])}\"\n",
    "                  )\n",
    "            i += 1\n",
    "            if abs(energy_log[-2] - energy_log[-1]) < converged_threshold:\n",
    "                count += 1\n",
    "                if count == 3:\n",
    "                    not_converged = False\n",
    "            else:\n",
    "                count = 0\n",
    "\n",
    "    def energy(self, eval_point: torch.Tensor = None, include_priors: bool = True) -> float:\n",
    "        \"\"\" Computes the sum of all of the squared errors in the graph using the appropriate local loss function. \"\"\"\n",
    "        if eval_point is None:\n",
    "            energy = sum([factor.get_energy() for factor in self.factors])\n",
    "        else:\n",
    "            var_dofs = torch.tensor([v.dofs for v in self.var_nodes])\n",
    "            var_ix = torch.cat([torch.tensor([0]), torch.cumsum(var_dofs, dim=0)[:-1]])\n",
    "            energy = 0.\n",
    "            for f in self.factors:\n",
    "                local_eval_point = torch.cat([eval_point[var_ix[v.variableID]: var_ix[v.variableID] + v.dofs] for v in f.adj_var_nodes])\n",
    "                energy += f.get_energy(local_eval_point)\n",
    "        if include_priors:\n",
    "            prior_energy = sum([var.get_prior_energy() for var in self.var_nodes])\n",
    "            energy += prior_energy\n",
    "        return energy\n",
    "\n",
    "    def get_joint_dim(self) -> int:\n",
    "        return sum([var.dofs for var in self.var_nodes])\n",
    "\n",
    "    def get_joint(self) -> Gaussian:\n",
    "        \"\"\"\n",
    "            Get the joint distribution over all variables in the information form\n",
    "            If nonlinear factors, it is taken at the current linearisation point.\n",
    "        \"\"\"\n",
    "        dim = self.get_joint_dim()\n",
    "        joint = Gaussian(dim)\n",
    "\n",
    "        # Priors\n",
    "        var_ix = [0] * len(self.var_nodes)\n",
    "        counter = 0\n",
    "        for var in self.var_nodes:\n",
    "            var_ix[var.variableID] = int(counter)\n",
    "            joint.eta[counter:counter + var.dofs] += var.prior.eta\n",
    "            joint.lam[counter:counter + var.dofs, counter:counter + var.dofs] += var.prior.lam\n",
    "            counter += var.dofs\n",
    "\n",
    "        # Other factors\n",
    "        for factor in self.factors:\n",
    "            factor_ix = 0\n",
    "            for adj_var_node in factor.adj_var_nodes:\n",
    "                vID = adj_var_node.variableID\n",
    "                # Diagonal contribution of factor\n",
    "                joint.eta[var_ix[vID]:var_ix[vID] + adj_var_node.dofs] += \\\n",
    "                    factor.factor.eta[factor_ix:factor_ix + adj_var_node.dofs]\n",
    "                joint.lam[var_ix[vID]:var_ix[vID] + adj_var_node.dofs, var_ix[vID]:var_ix[vID] + adj_var_node.dofs] += \\\n",
    "                    factor.factor.lam[factor_ix:factor_ix + adj_var_node.dofs, factor_ix:factor_ix + adj_var_node.dofs]\n",
    "                other_factor_ix = 0\n",
    "                for other_adj_var_node in factor.adj_var_nodes:\n",
    "                    if other_adj_var_node.variableID > adj_var_node.variableID:\n",
    "                        other_vID = other_adj_var_node.variableID\n",
    "                        # Off diagonal contributions of factor\n",
    "                        joint.lam[var_ix[vID]:var_ix[vID] + adj_var_node.dofs, var_ix[other_vID]:var_ix[other_vID] + other_adj_var_node.dofs] += \\\n",
    "                            factor.factor.lam[factor_ix:factor_ix + adj_var_node.dofs, other_factor_ix:other_factor_ix + other_adj_var_node.dofs]\n",
    "                        joint.lam[var_ix[other_vID]:var_ix[other_vID] + other_adj_var_node.dofs, var_ix[vID]:var_ix[vID] + adj_var_node.dofs] += \\\n",
    "                            factor.factor.lam[other_factor_ix:other_factor_ix + other_adj_var_node.dofs, factor_ix:factor_ix + adj_var_node.dofs]\n",
    "                    other_factor_ix += other_adj_var_node.dofs\n",
    "                factor_ix += adj_var_node.dofs\n",
    "\n",
    "        return joint\n",
    "\n",
    "    def MAP(self) -> torch.Tensor:\n",
    "        return self.get_joint().mean()\n",
    "\n",
    "    def dist_from_MAP(self) -> torch.Tensor:\n",
    "        return torch.norm(self.get_joint().mean() - self.belief_means())\n",
    "\n",
    "    def belief_means(self) -> torch.Tensor:\n",
    "        \"\"\" Get an array containing all current estimates of belief means. \"\"\"\n",
    "        return torch.cat([var.belief.mean() for var in self.var_nodes])\n",
    "\n",
    "    def belief_covs(self) -> List[torch.Tensor]:\n",
    "        \"\"\" Get a list containing all current estimates of belief covariances. \"\"\"\n",
    "        covs = [var.belief.cov() for var in self.var_nodes]\n",
    "        return covs\n",
    "\n",
    "    def get_gradient(self, include_priors: bool = True) -> torch.Tensor:\n",
    "        \"\"\" Return gradient wrt the total energy. \"\"\"\n",
    "        dim = self.get_joint_dim()\n",
    "        grad = torch.zeros(dim)\n",
    "        var_dofs = torch.tensor([v.dofs for v in self.var_nodes])\n",
    "        var_ix = torch.cat([torch.tensor([0]), torch.cumsum(var_dofs, dim=0)[:-1]])\n",
    "\n",
    "        if include_priors:\n",
    "            for v in self.var_nodes:\n",
    "                grad[var_ix[v.variableID]:var_ix[v.variableID] + v.dofs] += (v.belief.mean() - v.prior.mean()) @ v.prior.cov()\n",
    "\n",
    "        for f in self.factors:\n",
    "            r = f.get_residual()\n",
    "            jac = f.meas_model.jac_fn(f.linpoint)  # jacobian wrt residual\n",
    "            local_grad = (r @ torch.inverse(f.meas_model.loss.effective_cov) @ jac).flatten()\n",
    "\n",
    "            factor_ix = 0\n",
    "            for adj_var_node in f.adj_var_nodes:\n",
    "                vID = adj_var_node.variableID\n",
    "                grad[var_ix[vID]:var_ix[vID] + adj_var_node.dofs] += local_grad[factor_ix: factor_ix + adj_var_node.dofs]\n",
    "                factor_ix += adj_var_node.dofs\n",
    "        return grad\n",
    "\n",
    "    def gradient_descent_step(self, lr: float = 1e-3) -> None:\n",
    "        grad = self.get_gradient()\n",
    "        i = 0\n",
    "        for v in self.var_nodes:\n",
    "            v.belief.eta = v.belief.lam @ (v.belief.mean() - lr * grad[i: i+v.dofs])\n",
    "            i += v.dofs\n",
    "        self.linearise_all_factors()\n",
    "\n",
    "    def lm_step(self, lambda_lm: float, a: float=1.5, b: float=3) -> bool:\n",
    "        \"\"\" Very close to an LM step, except we always accept update even if it increases the energy.\n",
    "            As to compute the energy if we were to do the update, we would need to relinearise all factors.\n",
    "            Returns lambda parameters for LM.\n",
    "            If lambda_lm = 0, then it is Gauss-Newton.\n",
    "            \"\"\"\n",
    "        current_x = self.belief_means()\n",
    "        initial_energy = self.energy()\n",
    "\n",
    "        joint = self.get_joint()\n",
    "        A = joint.lam + lambda_lm * torch.eye(len(joint.eta))\n",
    "        b_mat = -self.get_gradient()\n",
    "        delta_x = torch.inverse(A) @ b_mat\n",
    "\n",
    "        i = 0  # apply update\n",
    "        for v in self.var_nodes:\n",
    "            v.belief.eta = v.belief.lam @ (v.belief.mean() + delta_x[i: i+v.dofs])\n",
    "            i += v.dofs\n",
    "        self.linearise_all_factors()\n",
    "        new_energy = self.energy()\n",
    "\n",
    "        if lambda_lm == 0.:  # Gauss-Newton\n",
    "            return lambda_lm\n",
    "        if new_energy < initial_energy:  # accept update\n",
    "            lambda_lm /= a\n",
    "            return lambda_lm\n",
    "        else:  # undo update\n",
    "            i = 0  # apply update\n",
    "            for v in self.var_nodes:\n",
    "                v.belief.eta = v.belief.lam @ (v.belief.mean() - delta_x[i: i+v.dofs])\n",
    "                i += v.dofs\n",
    "            self.linearise_all_factors()\n",
    "            lambda_lm = min(lambda_lm*b, 1e5)\n",
    "            return lambda_lm\n",
    "\n",
    "    def print(self, brief=False) -> None:\n",
    "        print(\"\\nFactor Graph:\")\n",
    "        print(f\"# Variable nodes: {len(self.var_nodes)}\")\n",
    "        if not brief:\n",
    "            for i, var in enumerate(self.var_nodes):\n",
    "                print(f\"Variable {i}: connects to factors {[f.factorID for f in var.adj_factors]}\")\n",
    "                print(f\"    dofs: {var.dofs}\")\n",
    "                print(f\"    prior mean: {var.prior.mean().numpy()}\")\n",
    "                print(f\"    prior covariance: diagonal sigma {torch.diag(var.prior.cov()).numpy()}\")\n",
    "        print(f\"# Factors: {len(self.factors)}\")\n",
    "        if not brief:\n",
    "            for i, factor in enumerate(self.factors):\n",
    "                if factor.meas_model.linear:\n",
    "                    print(\"Linear\", end =\" \")\n",
    "                else:\n",
    "                    print(\"Nonlinear\", end =\" \")\n",
    "                print(f\"Factor {i}: connects to variables {factor.adj_vIDs}\")\n",
    "                print(f\"    measurement model: {type(factor.meas_model).__name__},\"\n",
    "                    f\" {type(factor.meas_model.loss).__name__},\"\n",
    "                    f\" diagonal sigma {torch.diag(factor.meas_model.loss.effective_cov).detach().numpy()}\")\n",
    "                print(f\"    measurement: {factor.measurement.numpy()}\")\n",
    "        print(\"\\n\")\n",
    "    \n",
    "\n",
    "class VariableNode:\n",
    "    def __init__(self, id: int, dofs: int, properties: dict = {}) -> None:\n",
    "        self.variableID = id\n",
    "        self.properties = properties\n",
    "        self.dofs = dofs\n",
    "        self.adj_factors = []\n",
    "        self.belief = Gaussian(dofs)\n",
    "        self.prior = Gaussian(dofs)  # prior factor, implemented as part of variable node\n",
    "\n",
    "    def update_belief(self) -> None:\n",
    "        \"\"\" Update local belief estimate by taking product of all incoming messages along all edges. \"\"\"\n",
    "        self.belief.eta = self.prior.eta.clone()  # message from prior factor\n",
    "        self.belief.lam = self.prior.lam.clone()\n",
    "        for factor in self.adj_factors:  # messages from other adjacent variables\n",
    "            message_ix = factor.adj_vIDs.index(self.variableID)\n",
    "            self.belief.eta += factor.messages[message_ix].eta\n",
    "            self.belief.lam += factor.messages[message_ix].lam\n",
    "\n",
    "    def get_prior_energy(self) -> float:\n",
    "        energy = 0.\n",
    "        if self.prior.lam[0, 0] != 0.:\n",
    "            residual = self.belief.mean() - self.prior.mean()\n",
    "            energy += 0.5 * residual @ self.prior.lam @ residual\n",
    "        return energy\n",
    "\n",
    "\n",
    "class Factor:\n",
    "    def __init__(self,\n",
    "                 id: int,\n",
    "                 adj_var_nodes: List[VariableNode],\n",
    "                 measurement: torch.Tensor,\n",
    "                 meas_model: MeasModel,\n",
    "                 type: torch.dtype = torch.float,\n",
    "                 properties: dict = {}) -> None:\n",
    "\n",
    "        self.factorID = id\n",
    "        self.properties = properties\n",
    "\n",
    "        self.adj_var_nodes = adj_var_nodes\n",
    "        self.dofs = sum([var.dofs for var in adj_var_nodes])\n",
    "        self.adj_vIDs = [var.variableID for var in adj_var_nodes]\n",
    "        self.messages = [Gaussian(var.dofs) for var in adj_var_nodes]\n",
    "\n",
    "        self.factor = Gaussian(self.dofs)\n",
    "        self.linpoint = torch.zeros(self.dofs, dtype=type)\n",
    "\n",
    "        self.measurement = measurement\n",
    "        self.meas_model = meas_model\n",
    "\n",
    "        # For smarter GBP implementations\n",
    "        self.iters_since_relin = 0\n",
    "\n",
    "        self.compute_factor()\n",
    "\n",
    "    def get_adj_means(self) -> torch.Tensor:\n",
    "        adj_belief_means = [var.belief.mean() for var in self.adj_var_nodes]\n",
    "        return torch.cat(adj_belief_means)\n",
    "\n",
    "    def get_residual(self, eval_point: torch.Tensor = None) -> torch.Tensor:\n",
    "        \"\"\" Compute the residual vector. \"\"\"\n",
    "        if eval_point is None:\n",
    "            eval_point = self.get_adj_means()\n",
    "        return self.meas_model.meas_fn(eval_point) - self.measurement\n",
    "\n",
    "    def get_energy(self, eval_point: torch.Tensor = None) -> float:\n",
    "        \"\"\" Computes the squared error using the appropriate loss function. \"\"\"\n",
    "        residual = self.get_residual(eval_point)\n",
    "        # print(\"adj_belifes\", self.get_adj_means())\n",
    "        # print(\"pred and meas\", self.meas_model.meas_fn(self.get_adj_means()), self.measurement)\n",
    "        # print(\"residual\", self.get_residual(), self.meas_model.loss.effective_cov)\n",
    "        return 0.5 * residual @ torch.inverse(self.meas_model.loss.effective_cov) @ residual\n",
    "\n",
    "    def robust(self) -> bool:\n",
    "        return self.meas_model.loss.robust()\n",
    "\n",
    "    def compute_factor(self) -> None:\n",
    "        \"\"\"\n",
    "            Compute the factor at current adjacente beliefs using robust.\n",
    "            If measurement model is linear then factor will always be the same regardless of linearisation point.\n",
    "        \"\"\"\n",
    "        self.linpoint = self.get_adj_means()\n",
    "        J = self.meas_model.jac_fn(self.linpoint)\n",
    "        pred_measurement = self.meas_model.meas_fn(self.linpoint)\n",
    "        self.meas_model.loss.get_effective_cov(pred_measurement - self.measurement)\n",
    "        effective_lam = torch.inverse(self.meas_model.loss.effective_cov)\n",
    "        self.factor.lam = J.T @ effective_lam @ J\n",
    "        self.factor.eta = ((J.T @ effective_lam) @ (J @ self.linpoint + self.measurement - pred_measurement)).flatten()\n",
    "        self.iters_since_relin = 0\n",
    "\n",
    "    def robustify_loss(self) -> None:\n",
    "        \"\"\"\n",
    "            Rescale the variance of the noise in the Gaussian measurement model if necessary and update the factor\n",
    "            correspondingly.\n",
    "        \"\"\"\n",
    "        old_effective_cov = self.meas_model.loss.effective_cov[0, 0]\n",
    "        self.meas_model.loss.get_effective_cov(self.get_residual())\n",
    "        self.factor.eta *= old_effective_cov / self.meas_model.loss.effective_cov[0, 0]\n",
    "        self.factor.lam *= old_effective_cov / self.meas_model.loss.effective_cov[0, 0]\n",
    "\n",
    "    def compute_messages(self, damping: float = 0.) -> None:\n",
    "        \"\"\" Compute all outgoing messages from the factor. \"\"\"\n",
    "        messages_eta, messages_lam = [], []\n",
    "\n",
    "        start_dim = 0\n",
    "        for v in range(len(self.adj_vIDs)):\n",
    "            eta_factor, lam_factor = self.factor.eta.clone().double(), self.factor.lam.clone().double()\n",
    "\n",
    "            # Take product of factor with incoming messages\n",
    "            start = 0\n",
    "            for var in range(len(self.adj_vIDs)):\n",
    "                if var != v:\n",
    "                    var_dofs = self.adj_var_nodes[var].dofs\n",
    "                    eta_factor[start:start + var_dofs] += self.adj_var_nodes[var].belief.eta - self.messages[var].eta\n",
    "                    lam_factor[start:start + var_dofs, start:start + var_dofs] += self.adj_var_nodes[var].belief.lam - self.messages[var].lam\n",
    "                start += self.adj_var_nodes[var].dofs\n",
    "\n",
    "            # Divide up parameters of distribution\n",
    "            mess_dofs = self.adj_var_nodes[v].dofs\n",
    "            eo = eta_factor[start_dim:start_dim + mess_dofs]\n",
    "            eno = torch.cat((eta_factor[:start_dim], eta_factor[start_dim + mess_dofs:]))\n",
    "\n",
    "            loo = lam_factor[start_dim:start_dim + mess_dofs, start_dim:start_dim + mess_dofs]\n",
    "            lono = torch.cat((lam_factor[start_dim:start_dim + mess_dofs, :start_dim],\n",
    "                              lam_factor[start_dim:start_dim + mess_dofs, start_dim + mess_dofs:]), dim=1)\n",
    "            lnoo = torch.cat((lam_factor[:start_dim, start_dim:start_dim + mess_dofs],\n",
    "                              lam_factor[start_dim + mess_dofs:, start_dim:start_dim + mess_dofs]), dim=0)\n",
    "            lnono = torch.cat(\n",
    "                        (\n",
    "                            torch.cat((lam_factor[:start_dim, :start_dim], lam_factor[:start_dim, start_dim + mess_dofs:]), dim=1),\n",
    "                            torch.cat((lam_factor[start_dim + mess_dofs:, :start_dim], lam_factor[start_dim + mess_dofs:, start_dim + mess_dofs:]), dim=1)\n",
    "                        ),\n",
    "                        dim=0\n",
    "                    )\n",
    "\n",
    "            new_message_lam = loo - lono @ torch.inverse(lnono) @ lnoo\n",
    "            new_message_eta = eo - lono @ torch.inverse(lnono) @ eno\n",
    "            messages_eta.append((1 - damping) * new_message_eta + damping * self.messages[v].eta)\n",
    "            messages_lam.append((1 - damping) * new_message_lam + damping * self.messages[v].lam)\n",
    "            start_dim += self.adj_var_nodes[v].dofs\n",
    "\n",
    "        for v in range(len(self.adj_vIDs)):\n",
    "            self.messages[v].lam = messages_lam[v]\n",
    "            self.messages[v].eta = messages_eta[v]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
